{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1469d0f3ebc25d85be36d1786da14ff4f6ad1df"
   },
   "source": [
    "# Food101 Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d1a4776a4ab93cee4fc52eff262b722f68e078a"
   },
   "source": [
    "**Task:**<br>Food 101 is a labelled data set with 101 different food classes. Each food class contains 1000 images. Using the data provided,  a Machine Learning Model that can classify 3 classes in Food 101 dataset is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c747e2615610d9503b9b8b2ecdd4d54d027cdbd6"
   },
   "source": [
    "**The Setting:**\n",
    "<br>**Classes: **(Apple_pie, Baby_back_ribs, Baklava)\n",
    "<br>**Epoches: **100\n",
    "<br>**Batch_size:** 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Input, BatchNormalization, Multiply, Activation\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff72ab0a3f85b93f64d0365abcb71525bc69a160"
   },
   "source": [
    "#### Get the paths of the images and the train-test image files for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08c387fd59d1b6caba4d35198f696a8be2b6c92b"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Base folders\n",
    "train_path = r\"C:\\Users\\shett\\Downloads\\Food-Classification-master\\archive\\training\"\n",
    "test_path  = r\"C:\\Users\\shett\\Downloads\\Food-Classification-master\\archive\\testing\"  # if you have testing folder\n",
    "\n",
    "# Classes\n",
    "food = ['Vegetable-Fruit', 'Soup', 'Seafood']\n",
    "\n",
    "train_path = r\"C:\\Users\\shett\\Downloads\\Food-Classification-master\\archive\\training\"\n",
    "test_path  = r\"C:\\Users\\shett\\Downloads\\Food-Classification-master\\archive\\testing\"\n",
    "\n",
    "f_veg   = glob.glob(os.path.join(train_path, 'Vegetable-Fruit', \"*\"))\n",
    "f_soup  = glob.glob(os.path.join(train_path, 'Soup', \"*\"))\n",
    "f_sea   = glob.glob(os.path.join(train_path, 'Seafood', \"*\"))\n",
    "\n",
    "print('Number of images per class:')\n",
    "print(f'Vegetable-Fruit: {len(f_veg)}')\n",
    "print(f'Soup: {len(f_soup)}')\n",
    "print(f'Seafood: {len(f_sea)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06baec819f8d09d46d7f124a7c72fce84a4cca0f"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c0955407411485c053ef601babc3836c974b4e1"
   },
   "source": [
    "#### Let's get an idea of the images of the 3 categories. As we see here, the quality of the images are not very good: with different background (noise), different lightings and even wrong labels (e.g. empty plate in the first pic of the baklava, missing apple pie in the last apple pie pic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "79ff5c6411bb384381c74f383841323b731ef5e3"
   },
   "outputs": [],
   "source": [
    "# preview some images of each class\n",
    "n = min(7, len(f_veg), len(f_soup), len(f_sea))  # ensure we don't exceed available images\n",
    "fig, axes = plt.subplots(3, n, figsize=(20, 10))\n",
    "\n",
    "for i in range(n):\n",
    "    axes[0, i].imshow(plt.imread(f_veg[i]))\n",
    "    axes[0, i].set_title('Vegetable-Fruit')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(plt.imread(f_soup[i]))\n",
    "    axes[1, i].set_title('Soup')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    axes[2, i].imshow(plt.imread(f_sea[i]))\n",
    "    axes[2, i].set_title('Seafood')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ff841b7c83ddfc266e731f47bee3c4b19676f5c"
   },
   "source": [
    "#### The images are of different sizes and aspect ratio, with at least one side of 512 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75afa9ca0c67306637411855d7ab9adc46fbf09a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_len = min(len(f_veg), len(f_soup), len(f_sea))\n",
    "\n",
    "for i in range(min_len):\n",
    "    h1, w1, c1 = plt.imread(f_veg[i]).shape\n",
    "    h2, w2, c2 = plt.imread(f_soup[i]).shape\n",
    "    h3, w3, c3 = plt.imread(f_sea[i]).shape\n",
    "    \n",
    "    plt.scatter(h1, w1, c='r', marker='x', alpha=0.5)\n",
    "    plt.scatter(h2, w2, c='c', marker='o', alpha=0.5)\n",
    "    plt.scatter(h3, w3, c='b', marker='v', alpha=0.5)\n",
    "\n",
    "plt.title('Image Size')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Width')\n",
    "plt.legend(('Vegetable-Fruit','Soup','Seafood'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d2b20091ef6094ef6f1d5eb681aa968050826d3"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78456644b7c68a9cc58d5696e8d0146994683488"
   },
   "source": [
    "#### In order to avoid overfitting problem and to expand the dataset. Image data generator from Keras is used for image tranformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a798af17e7aa25a3ccf0fd65760b6ee761b79a35"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation for training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=1/255.0\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"../input/food102/food101/food101/train\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Only rescaling for test set\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"../input/food102/food101/food101/test\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc8926f6b2e8ebf64e82e890a904b50936991d34"
   },
   "source": [
    "#### Just to make sure the image generator is working and the transformation is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93836a2f373f93ca25e72dfad0bd771ab2002ef3"
   },
   "outputs": [],
   "source": [
    "# preview images from train generator\n",
    "r = 4; c = 7\n",
    "n=0\n",
    "classtolabel = {'0':'apple_pie','1':'baby_pork_ribs','2':'baklava'}\n",
    "for x in train_generator:\n",
    "    fig, axes = plt.subplots(r,c,figsize=(20,12))\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axes[i,j].imshow(x[0][n])\n",
    "            label = np.argmax(x[1],axis=1)[n].astype('str')\n",
    "            axes[i,j].set_title(classtolabel[label])\n",
    "            n+=1    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "778d2276f930f6fa7e5145d8396f8fc758e4fe83"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b02e0cca7ce4a81cd71b1c76353ba44843da7b6a"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu', input_shape = (224,224,3), kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation = \"relu\",kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation = \"softmax\",kernel_initializer='he_normal',kernel_regularizer=l2()))\n",
    "\n",
    "#callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='model.hdf5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, mode='auto')\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, mode='auto')\n",
    "\n",
    "model.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f01c96019f2f0d1cbf0bba639c3bfb042008cb0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2854a526419418b5debdad857081a014ca91d7c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator,steps_per_epoch=2250/64,\n",
    "                              validation_data=test_generator,validation_steps=750/64, \n",
    "                              epochs=100, callbacks=[checkpointer, reduceLR, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88b466ef6dcfd6436c84d0b91381c04fb0190636"
   },
   "outputs": [],
   "source": [
    "# # load weights from upload files\n",
    "# model.load_weights('../input/model-v17/model_v17.hdf5')\n",
    "\n",
    "# load weights from training with lowest val_loss\n",
    "# model.load_weights('../working/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cad5fa8401699aea9e539dd1009d4e1644095734"
   },
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    f,ax = plt.subplots(2,1,figsize=(15,10))\n",
    "    ax[0].plot(history.history['acc'],c='C2')\n",
    "    ax[0].plot(history.history['val_acc'],c='C3')\n",
    "    ax[0].set_title('Model accuracy')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    ax[1].plot(history.history['loss'],c='C0')\n",
    "    ax[1].plot(history.history['val_loss'],c='C1')\n",
    "    ax[1].set_title('Model loss')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b2d7f6258fe6574bf38b00c6c566533d4ae7def"
   },
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd1081f969da260c4ff4e90ab8414e6c4dd28bd8"
   },
   "outputs": [],
   "source": [
    "# create another generator for all test images in a single batch \n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "        \"../input/food102/food101/food101/test\",\n",
    "        target_size=(224,224),\n",
    "        batch_size=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6db6d710ef2bb1416713355b225d33dc02915fc"
   },
   "outputs": [],
   "source": [
    "x_test, y_test = val_generator.next()\n",
    "y_pred_conf = model.predict(x_test) #return probabilities of each class\n",
    "y_pred = np.argmax(y_pred_conf,axis=1)\n",
    "y_label = np.argmax(y_test,axis=1)\n",
    "\n",
    "print('Accuracy score: {:.1f}%'.format(accuracy_score(y_pred,y_label)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15e347c0c5c7808bf931b118abfe7c7ff5b0163c"
   },
   "source": [
    "### Randomly check 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ed934084dbfcc0fdbe1e5ce22da9addc2b80408"
   },
   "outputs": [],
   "source": [
    "ind = np.random.randint(1,len(x_test),5)\n",
    "f, ax=plt.subplots(1,5,figsize=(20,10))\n",
    "for i,j in enumerate(ind):\n",
    "    ax[i].imshow(x_test[j])\n",
    "    ax[i].set_title(\"Pred :{}({:.2f})\\nTrue :{}({:.2f})\".format\n",
    "                          (classtolabel[str(y_pred[j])],np.max(y_pred_conf[j]),\n",
    "                           classtolabel[str(y_label[j])],y_pred_conf[j][(y_label[j])],fontweight=\"bold\", size=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e096b87cbb72df95a8157bf787c0d457ff7cbe5"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "284072785baaad626f411435f8a484e834feec65"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix',fontsize=15)\n",
    "    plt.colorbar()\n",
    "    classes = ['apple_pie','baby_pork_ribs','baklava']\n",
    "    plt.xticks([0,1,2], classes, fontsize=10)\n",
    "    plt.yticks([0,1,2], classes, fontsize=10,rotation=90,verticalalignment=\"center\")\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > np.max(cm)/2. else \"black\")\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    \n",
    "plot_confusion_matrix(confusion_matrix(y_label,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fc9bcd37a545325ac288221eb37b6625ae6e8cf"
   },
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb18074fad87f93993e09a9d52d0187a084bb529"
   },
   "outputs": [],
   "source": [
    "fpr = dict() # false positive rate\n",
    "tpr = dict() # true positive rate\n",
    "roc_auc = dict() # area under roc curve\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_conf[:, i]) # roc_curve function apply to binary class only\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])  # using the trapezoidal rule to get area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f604bfc8de26228fc1bc1e65756a3641bd5c40c6"
   },
   "outputs": [],
   "source": [
    "def plot_roc(fpr,tpr,roc_auc):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(fpr[0], tpr[0], color='C1', lw=3, label='ROC curve of apple_pie (AUC = %0.2f)' % roc_auc[0])\n",
    "    plt.plot(fpr[1], tpr[1], color='C2', lw=3, label='ROC curve of baby_pork_ribs (AUC = %0.2f)' % roc_auc[1])\n",
    "    plt.plot(fpr[2], tpr[2], color='C3', lw=3, label='ROC curve of baklava (AUC = %0.2f)' % roc_auc[2])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--',alpha=0.7)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate',fontsize=20)\n",
    "    plt.ylabel('True Positive Rate',fontsize=20)\n",
    "    plt.title('Receiver Operating Characteristics Curve',fontsize=30)\n",
    "    plt.legend(loc=\"lower right\",fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "plot_roc(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a90f1e2d3cae924586d442c4bdd3273b6eea9fa"
   },
   "source": [
    "### Inspect the predictions with wrong labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7d7b65ab10923ae91229f762484150cb3f0570a"
   },
   "outputs": [],
   "source": [
    "# find the wrong-est label (largest confidence wrong label)\n",
    "def show_wrongest_label(x_test,y_test,y_pred_conf):\n",
    "    y_pred = np.argmax(y_pred_conf,axis=1) # convert predictions to labels\n",
    "    y_label = np.argmax(y_test,axis=1) # convert answer to labels\n",
    "\n",
    "    errors = (y_pred - y_label != 0) # find booleans of wrong predictions\n",
    "    y_pred_errors = y_pred_conf[errors] #the probabilities of the wrong Y_pred [0.5,0.2,0.3]\n",
    "\n",
    "    y_pred_classes_errors = y_pred[errors] # the wrong pred label [2]\n",
    "    y_pred_errors_prob = np.max(y_pred_errors,axis = 1) # Probabilities of the wrong predicted numbers [0.5]\n",
    "\n",
    "    y_true_classes_errors = y_label[errors] # the true label [0]\n",
    "    y_true_errors_prob = np.diagonal(np.take(y_pred_errors, y_true_classes_errors, axis=1)) # Predicted prob of the true values in the error set[0.2]\n",
    "\n",
    "    img_errors = x_test[errors] # image of each errors\n",
    "\n",
    "    # Difference between the probability of the predicted label and the true label\n",
    "    delta_pred_true_errors = y_pred_errors_prob - y_true_errors_prob\n",
    "    # Get index of delta prob errors in ascending order\n",
    "    sorted_delta_errors = np.argsort(delta_pred_true_errors)\n",
    "    # The index of top 15 errors \n",
    "    most_important_errors = sorted_delta_errors[-15:]\n",
    "    \n",
    "    \n",
    "    def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "        n = 0\n",
    "        nrows = 3\n",
    "        ncols = 5\n",
    "        fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "        fig.set_figheight(20)\n",
    "        fig.set_figwidth(30)\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                error = errors_index[n]\n",
    "                ax[row,col].imshow((img_errors[error]))\n",
    "                ax[row,col].set_title(\"Pred :{}({:.2f})\\nTrue :{}({:.2f})\".format\n",
    "                                      (classtolabel[pred_errors[error].astype('str')],y_pred_errors_prob[error],\n",
    "                                       classtolabel[obs_errors[error].astype('str')],y_true_errors_prob[error]),\n",
    "                                      fontweight=\"bold\", size=20)\n",
    "                n += 1\n",
    "    \n",
    "    display_errors(most_important_errors, img_errors, y_pred_classes_errors, y_true_classes_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b239e8f2a9d5824d91d6d22d179fceb910d8ce7"
   },
   "outputs": [],
   "source": [
    "show_wrongest_label(x_test,y_test,y_pred_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47b0663508e8eb789beaeb13853af07a3ba297be",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6fa4a795f8161cf38a00b3a855c1f3358ef71d38"
   },
   "source": [
    "# Gradient weighted Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dae9bb7ae2c32f5dd705c10f331029bbc7c0539"
   },
   "outputs": [],
   "source": [
    "# define last convolution layer\n",
    "last_conv_layer = model.layers[-5]\n",
    "\n",
    "\n",
    "def get_grad_cam(img): # function for getting the gradient class attention map\n",
    "    # predict class of the image\n",
    "    y_preds = model.predict(np.expand_dims(img,axis=0))\n",
    "    y_pred_class = np.argmax(y_pred[0])\n",
    "    # prediction vector of the predicted class\n",
    "    class_output = model.output[:, y_pred_class]\n",
    "    # gradient of the predicted class with regard to the output feature map from last conv layer\n",
    "    # shape of (-1,7,7,256)\n",
    "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "    # vector of shape (256,)\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    # create backend function to get values when input image\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([np.expand_dims(img,axis=0)])\n",
    "    # multiplies each channel in the feature map by 'how important this channel is' \n",
    "    # regard to the predicted class\n",
    "    for i in range(256):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    # to visualise (7, 7, 256), mean over all channels\n",
    "    gcam = np.mean(conv_layer_output_value, axis=-1)\n",
    "    \n",
    "    # gcam = minmax_scaling(gcam,columns=[0,1,2,3,4,5,6])\n",
    "    \n",
    "    # ignore all negative values\n",
    "    gcam = np.maximum(gcam, 0)\n",
    "    # normalise to [0-1] scale\n",
    "    gcam /= np.max(gcam)\n",
    "    # resize to original size image (7,7) to (224,224)\n",
    "    \n",
    "    gcam = scipy.ndimage.zoom(gcam, (224/7,224/7), order=1)\n",
    "    return gcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb5dd6a29b970c03a3002983beb0c3f157e0f1b2"
   },
   "outputs": [],
   "source": [
    "f, ax=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "n=50\n",
    "\n",
    "ax[0].imshow(x_test[n],alpha=0.4)\n",
    "ax[0].imshow(get_grad_cam(x_test[n]),alpha=0.6)\n",
    "ax[1].imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbe70db57d93b835f0ca939ef8f309c7eab8e8a"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
